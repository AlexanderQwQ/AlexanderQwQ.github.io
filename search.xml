<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Pytorch Entry</title>
    <url>/2024/10/31/conda/</url>
    <content><![CDATA[<h2 id="详细内容教学视频请移步"><a href="#详细内容教学视频请移步" class="headerlink" title="详细内容教学视频请移步"></a>详细内容教学视频请移步</h2><p><a href="https://www.bilibili.com/video/BV1hE411t7RN/">PyTorch深度学习快速入门教程（绝对通俗易懂！）【小土堆】</a> </p>
<h2 id="更简单易懂的请参考龙哥的Blog"><a href="#更简单易懂的请参考龙哥的Blog" class="headerlink" title="更简单易懂的请参考龙哥的Blog"></a>更简单易懂的请参考龙哥的Blog</h2><p><a href="https://kuaimiaoda.cn/cuda/">pytorch+CUDA 安装教程</a></p>
<h2 id="Anaconda环境的创建和切换"><a href="#Anaconda环境的创建和切换" class="headerlink" title="Anaconda环境的创建和切换"></a>Anaconda环境的创建和切换</h2><ol>
<li>打开 Win-&gt;Anaoconda3-&gt;Anaconda Prompt</li>
<li>命令行 conda create -n <strong>pytorch(名字)</strong> python&#x3D;3.6</li>
<li>激活环境 conda activate <strong>pytorch(名字)</strong></li>
</ol>
<h2 id="Cuda环境的配置和安装"><a href="#Cuda环境的配置和安装" class="headerlink" title="Cuda环境的配置和安装"></a>Cuda环境的配置和安装</h2><ol>
<li>查看GPU型号 nvidia-smi</li>
<li>检验 python,import torch,torch.cuda.is_availiable()</li>
<li>返回 True</li>
</ol>
<h2 id="Python学习的两大法宝"><a href="#Python学习的两大法宝" class="headerlink" title="Python学习的两大法宝"></a>Python学习的两大法宝</h2><ol>
<li>dir(pytorch.__):用来打开某一个包，展示出内部分区。</li>
<li>help(__):用来查看该分区内部某一工具的使用说明。</li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>Background of Graduation Project</title>
    <url>/2024/11/04/background/</url>
    <content><![CDATA[<h2 id="放射性骨损伤的研究进展"><a href="#放射性骨损伤的研究进展" class="headerlink" title="放射性骨损伤的研究进展"></a><a href="http://www.ijrmnm.com/article/doi/10.3760/cma.j.cn121381-202007046-00093?viewType=HTML">放射性骨损伤的研究进展</a></h2><h2 id="影响因素"><a href="#影响因素" class="headerlink" title="影响因素"></a>影响因素</h2><p>患者在接受放射治疗之后，可能会出现骨质疏松的情况。</p>
<p>此类治疗引起骨质疏松的机制主要如下： </p>
<ol>
<li>放疗可能会导致内分泌的紊乱。患者接受放射治疗之后，可能会直接是内分泌的相关腺体受到影响，继而使雌激素雄激素生长激素这类的激素分泌不足，就有可能会导致骨代谢失衡，进而产生骨质疏松的情况。</li>
<li>患者进行放射治疗之后可能会产生胃肠道相关的反应，比如长期存在恶心呕吐或者消化不良的情况，就会引起钙质，蛋白质，维生素等营养物质的吸收不足，也会导致骨质疏松。</li>
<li>进行放射治疗还有可能会直接导致局部的骨细胞产生坏死，此时可能会使局部并发骨质疏松。</li>
<li>由于放疗中杀死骨细胞和骨髓细胞造成钙物质大量流失骨结构改变。此时患者表现为局部肿胀疼痛、四肢无力的症状。</li>
</ol>
<h2 id="病状体征"><a href="#病状体征" class="headerlink" title="病状体征"></a>病状体征</h2><ol>
<li>骨损伤的程度与放射源性质、照射剂量、剂量率、照射次数、间隔时间、照射部位及范围等因素有关。照射剂量大、间隔时间短、范围大者出现时间早、程度重，一次大剂量照射比分次小剂量照射损伤重。</li>
<li>骨质疏松、骨髓炎、病理骨折、骨坏死是损伤的一个发展演变过程，骨损伤程度和X射线征象的变化与受照射剂量、照射后的时间相一致。同时与受照射局部的处理和保护是否得当也有关。</li>
</ol>
<h2 id="放射性骨损伤的研究进展-1"><a href="#放射性骨损伤的研究进展-1" class="headerlink" title="放射性骨损伤的研究进展"></a><a href="http://www.ijrmnm.com/article/doi/10.3760/cma.j.cn121381-202007046-00093?viewType=HTML">放射性骨损伤的研究进展</a></h2><h2 id="治疗方案"><a href="#治疗方案" class="headerlink" title="治疗方案"></a>治疗方案</h2><ol>
<li>需要立即进行骨密度测试，诊断为骨质疏松后患者立即进行治疗。以非甾体抗炎镇痛药物为主。</li>
<li>患者需要改善生活方式，补充钙物质维生素消炎镇痛时症状尽快改善。建议患者通过补充碳酸钙消改变症状，促进骨骼的强壮。</li>
<li>为预防和减轻放射性骨损伤的发生，应给予富含钙和蛋白质的饮食，注意适当活动。</li>
<li>对已确定局部受照剂量超过骨损伤的参考阈剂量，无论有无骨损伤的临床或X线表现，均应脱离射线，凡出现骨损伤者，更应脱离放射线，或视全身情况改为非放射性工作。</li>
<li>应用改善微循环和促进骨组织修复、再生的药物：如复方丹参、谷胱甘肽、抗坏血酸、降钙素、维生素A、维生素D、康力龙等蛋白同化激素，以及含钙制剂药物。</li>
<li>注意避免骨损伤部位遭到外伤或感染，避免活检，皮肤出现明显萎缩或溃疡时应及时处理并采取手术治疗，用血循环良好的皮瓣或肌皮瓣覆盖，以改善局部的血液循环，消除创面。</li>
<li>发生骨髓炎时，应给予抗感染治疗，并及时采取手术治疗，彻底清除坏死骨，以带血管蒂的肌皮瓣充填腔穴和修复创面。</li>
<li>单个指骨或趾骨出现骨髓炎时，应及时截指(趾)，如累积多个指(趾)而保留剩余个别指(趾)已无功能时，可考虑截肢，但应慎重。截肢高度应超过损伤的近端3—5cm。</li>
</ol>
<h2 id="骨质酥松的介绍"><a href="#骨质酥松的介绍" class="headerlink" title="骨质酥松的介绍"></a>骨质酥松的介绍</h2><p>骨质酥松（Osteoporosis）是一种以骨密度降低和骨组织微结构破坏为特征的系统性骨骼疾病，导致骨骼强度降低，增加骨折风险。其主要特点包括： </p>
<ol>
<li>骨量减少：骨骼中的矿物质和有机成分减少，导致骨量降低。</li>
<li>骨组织微结构变化：骨小梁的数量减少和质量下降，使得骨骼的承载能力下降。</li>
<li>骨折风险增加：骨质酥松使得骨骼在轻微外力作用下（如跌倒）更容易发生骨折，尤其是髋部、脊柱和腕部。</li>
<li>无明显症状：在早期，骨质酥松通常没有明显的症状，常常在骨折发生后才被发现。<br>骨质酥松的风险因素包括年龄、性别（女性尤为常见）、家族历史、激素水平变化、营养不良（如钙和维生素D不足）、缺乏运动等。预防和管理措施包括合理饮食、定期锻炼、药物治疗等。</li>
</ol>
<h2 id="放疗的机制"><a href="#放疗的机制" class="headerlink" title="放疗的机制"></a>放疗的机制</h2><p>放疗（放射治疗）是利用高能辐射来杀死癌细胞或抑制其生长的一种治疗方法。其机制可以分为以下几个方面： </p>
<ol>
<li>辐射类型</li>
</ol>
<ul>
<li>外照射（External Beam Radiation Therapy, EBRT）：使用线性加速器产生的高能X射线或质子束，对肿瘤进行照射。</li>
<li>内照射（Brachytherapy）：将放射性物质直接植入或靠近肿瘤组织，以提供局部的高剂量辐射。</li>
</ul>
<ol start="2">
<li>辐射作用机制</li>
</ol>
<ul>
<li>细胞损伤：辐射直接损伤癌细胞的DNA。辐射能量产生自由基，这些自由基会攻击细胞内的DNA分子，造成单链或双链断裂，影响细胞的正常分裂和功能。辐射也通过与细胞周围的水分子反应产生自由基，这些自由基进一步损伤细胞的DNA。</li>
<li>细胞死亡：细胞周期依赖性：癌细胞在分裂阶段（如有丝分裂）对辐射更加敏感，损伤DNA后无法正常进行细胞分裂，最终导致细胞死亡。凋亡（Apoptosis）：DNA损伤可引发细胞凋亡程序，激活细胞内部的自毁机制，导致细胞有序死亡。</li>
</ul>
<ol start="3">
<li>影响因素</li>
</ol>
<ul>
<li>辐射剂量：高剂量的辐射能够更有效地杀死癌细胞，但也可能增加对周围正常组织的损伤。</li>
<li>照射时间：分次照射可以让正常细胞有机会修复受损，而癌细胞修复能力相对较弱。</li>
<li>组织类型：不同类型的组织对辐射的敏感性不同，肿瘤细胞和正常细胞的响应也各异。</li>
</ul>
<ol start="4">
<li>副作用</li>
</ol>
<ul>
<li>局部皮肤反应：如红肿、脱皮和疼痛。</li>
<li>组织损伤：如放射性肺炎、放射性肠炎等，取决于照射部位。</li>
<li>骨质酥松：尤其在照射骨骼或邻近组织时，可能导致骨密度降低。</li>
</ul>
<ol start="5">
<li>结合治疗<br>放疗通常与其他治疗方法（如手术、化疗、免疫治疗）结合使用，以提高治疗效果。多模态治疗可以增强对癌细胞的杀伤，同时减少单一治疗的副作用。</li>
</ol>
<h2 id="放疗相关骨质酥松的风险因素"><a href="#放疗相关骨质酥松的风险因素" class="headerlink" title="放疗相关骨质酥松的风险因素"></a>放疗相关骨质酥松的风险因素</h2><ol>
<li>治疗相关因素</li>
</ol>
<ul>
<li>照射区域：放疗针对骨密度高的区域（如脊柱、骨盆、四肢）时，风险更高。这些部位的骨骼更容易受到辐射损伤，导致骨质流失。</li>
<li>治疗剂量：高剂量的放疗对骨组织的损害更明显，尤其是当剂量超过某个阈值时，会显著影响骨代谢。</li>
<li>治疗频率和持续时间：长时间或频繁的放疗可能增加骨质酥松的风险，因为这会导致累积性损伤。</li>
<li>放疗技术：不同的放疗技术（如立体定向放疗与传统放疗）可能对骨组织的影响不同。一些现代技术能够更精确地集中照射肿瘤，减少对周围健康骨骼的影响。</li>
</ul>
<ol start="2">
<li>患者个体因素</li>
</ol>
<ul>
<li>年龄：年长患者的骨质通常已较为脆弱，放疗可能加重骨质流失的风险。</li>
<li>性别：女性由于激素水平变化（如绝经后雌激素减少）而更易发生骨质酥松，因此在接受放疗后风险更高。</li>
<li>基础健康状况：如果患者在放疗前已有骨质疏松或其他影响骨骼健康的疾病（如类风湿关节炎、甲状腺疾病等），则放疗后的骨质酥松风险会增加。</li>
<li>营养状况：钙和维生素D摄入不足会影响骨骼健康，使得放疗后的骨质酥松风险加大。</li>
<li>生活方式：缺乏运动、吸烟和酗酒等不良生活习惯会进一步增加骨质酥松的风险。</li>
</ul>
<ol start="3">
<li>药物治疗</li>
</ol>
<ul>
<li>激素类药物：一些癌症患者可能需要长期使用类固醇等激素类药物，这些药物本身就可能导致骨质流失。</li>
<li>抗肿瘤药物：某些化疗药物和靶向治疗药物也可能对骨骼产生不利影响，进一步增加骨质酥松的风险。</li>
</ul>
<ol start="4">
<li>精神心理因素<br>压力和焦虑：癌症患者的心理状态可能影响身体的整体健康，包括骨密度，长期的心理压力可能对骨代谢产生负面影响。</li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>linuxtips</title>
    <url>/2025/10/05/linuxtips/</url>
    <content><![CDATA[<h1 id="VMware的使用方法"><a href="#VMware的使用方法" class="headerlink" title="VMware的使用方法"></a>VMware的使用方法</h1><h3 id="1-虚拟机内快捷键"><a href="#1-虚拟机内快捷键" class="headerlink" title="1.虚拟机内快捷键"></a>1.虚拟机内快捷键</h3><ul>
<li>Ctrl + Alt 虚拟机内与主机切换</li>
<li>Ctrl + Alt + T 虚拟机内打开终端</li>
<li>sync 将内存中的数据写入硬盘</li>
<li>shutdown 关机</li>
<li>reboot 重启</li>
</ul>
<h3 id="2-系统目录结构"><a href="#2-系统目录结构" class="headerlink" title="2.系统目录结构"></a>2.系统目录结构</h3><p>对如下目录项的解释：</p>
<ul>
<li>&#x2F;bin 存放系统命令的目录</li>
<li>&#x2F;boot 存放系统引导文件的目录</li>
<li>&#x2F;dev 存放设备文件的目录</li>
<li>&#x2F;etc 存放系统配置文件的目录</li>
<li>&#x2F;home 存放用户文件的目录</li>
<li>&#x2F;lib 存放系统库文件的目录</li>
<li>&#x2F;mnt 存放临时挂载的文件系统的目录</li>
<li>&#x2F;opt 存放可选的软件包的目录</li>
<li>&#x2F;root 存放超级用户文件的目录</li>
<li>&#x2F;usr 存放用户程序的目录</li>
<li>&#x2F;tmp 存放临时文件的目录</li>
<li>&#x2F;var 存放变量日志文件的目录</li>
</ul>
<h3 id="3-常用命令"><a href="#3-常用命令" class="headerlink" title="3.常用命令"></a>3.常用命令</h3><ul>
<li>cd 切换目录</li>
<li>cd &#x2F; 进入根目录</li>
<li>cd .. 返回上一级目录</li>
<li>ls 查看当前目录下的文件</li>
<li>ls -l 查看当前目录下的文件详细信息</li>
<li>ls -a 查看当前目录下的所有文件，包括隐藏文件(l和a可以组合使用)</li>
<li>pwd 查看当前目录的绝对路径</li>
<li>mkdir 创建目录</li>
<li>mkdir -p 递归创建目录</li>
<li>rmdir 删除·空·目录</li>
<li>cp source target 复制文件从source到target（可以是文件也可以是目录）（可重写）</li>
<li>mv -f 强制移动文件或目录</li>
<li>mv -u 目标文件存在则更新目标文件</li>
<li>mv 可以重命名文件或目录</li>
<li>rm -f 忽略不存在的文件，强制删除文件</li>
<li>rm -r 递归删除目录</li>
<li>rm -i 删除前确认（有互动操作）</li>
<li>rm -rf &#x2F; 强制删除根目录下的所有文件（删库跑路）</li>
</ul>
]]></content>
  </entry>
  <entry>
    <title>Deeplearning</title>
    <url>/2024/10/31/deeplearning/</url>
    <content><![CDATA[<h2 id="DataSet-的相关内容"><a href="#DataSet-的相关内容" class="headerlink" title="DataSet 的相关内容"></a>DataSet 的相关内容</h2><ol>
<li><p>Dataset: PyTorch的数据集类 </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset  <span class="comment"># Dataset: PyTorch的数据集类</span></span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image  <span class="comment"># PIL: Python的图像处理库，Pillow是PIL的一个分支</span></span><br><span class="line"><span class="keyword">import</span> os  <span class="comment"># os: Python的标准库，提供了丰富的方法来处理文件和目录</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyData</span>(<span class="title class_ inherited__">Dataset</span>):  <span class="comment"># 继承Dataset类</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, root_dir, lable_dir</span>):  <span class="comment"># 初始化函数</span></span><br><span class="line">        <span class="variable language_">self</span>.root_dir = root_dir  <span class="comment"># 数据集的根目录</span></span><br><span class="line">        <span class="variable language_">self</span>.lable_dir = lable_dir  <span class="comment"># 数据集的标签</span></span><br><span class="line">        <span class="variable language_">self</span>.path = os.path.join(<span class="variable language_">self</span>.root_dir, <span class="variable language_">self</span>.lable_dir)  <span class="comment"># 数据集的路径</span></span><br><span class="line">        <span class="variable language_">self</span>.img_path = os.listdir(<span class="variable language_">self</span>.path)  <span class="comment"># 数据集的图片路径 返回指定的文件夹包含的文件或文件夹的名字的列表</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):  <span class="comment"># 返回数据集的长度</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.img_path)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):  <span class="comment"># 返回数据集的一个样本</span></span><br><span class="line">        img_name = <span class="variable language_">self</span>.img_path[idx]  <span class="comment"># 获取图片的名称</span></span><br><span class="line">        img_item_path = os.path.join(<span class="variable language_">self</span>.path, img_name)  <span class="comment"># 获取图片的路径</span></span><br><span class="line">        img = Image.<span class="built_in">open</span>(img_item_path)  <span class="comment"># 读取图片</span></span><br><span class="line">        lable = <span class="variable language_">self</span>.lable_dir  <span class="comment"># 获取图片的标签</span></span><br><span class="line">        <span class="keyword">return</span> img, lable  <span class="comment"># 返回图片和标签</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">root_dir = <span class="string">&#x27;dataset/train&#x27;</span>  <span class="comment"># 数据集的根目录</span></span><br><span class="line">ants_root_dir = <span class="string">&#x27;ants_image&#x27;</span>  <span class="comment"># 蚂蚁的数据集</span></span><br><span class="line">bees_root_dir = <span class="string">&#x27;bees_image&#x27;</span>  <span class="comment"># 蜜蜂的数据集</span></span><br><span class="line">ants_dataset = MyData(root_dir, ants_root_dir)  <span class="comment"># 蚂蚁的数据集</span></span><br><span class="line">bees_dataset = MyData(root_dir, bees_root_dir)  <span class="comment"># 蜜蜂的数据集</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>据训练集生成标签集lable</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">root_dir = <span class="string">&quot;dataset/train&quot;</span></span><br><span class="line">target_dir = <span class="string">&quot;bees_image&quot;</span></span><br><span class="line">img_path = os.listdir(os.path.join(root_dir, target_dir))</span><br><span class="line">lable = target_dir.split(<span class="string">&#x27;_&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line">out_dir = <span class="string">&quot;bees_lable&quot;</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> img_path:</span><br><span class="line">    file_name = i.split(<span class="string">&#x27;.jpg&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(os.path.join(root_dir, out_dir, <span class="string">&quot;&#123;&#125;.txt&quot;</span>.<span class="built_in">format</span>(file_name)), <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        f.write(lable)</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="TensorBoard-的相关操作"><a href="#TensorBoard-的相关操作" class="headerlink" title="TensorBoard 的相关操作"></a>TensorBoard 的相关操作</h2><ol>
<li>如何打开Tensorboard 的logs事件文件,<strong>port是指定打开的端口号，默认是6006</strong> documentation<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">tensorboard <span class="literal">--logdir</span>=logs <span class="literal">--port</span>=<span class="number">6006</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter  <span class="comment"># Summary Writer是一个用于写入TensorBoard的类</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;logs&#x27;</span>)  <span class="comment"># 创建一个SummaryWriter对象，指定存储路径写入logs文件夹事件文件</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## PIL图像类型转为numpy类型</span></span><br><span class="line">img_PIL = Image.<span class="built_in">open</span>(<span class="string">&quot;dataset/train/ants_image/0013035.jpg&quot;</span>)  <span class="comment"># 打开一张图片</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(img_PIL))</span><br><span class="line">img_array = np.array(img_PIL)  <span class="comment"># 将PIL图像转为numpy数组</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(img_array))</span><br><span class="line"></span><br><span class="line">writer.add_image(<span class="string">&quot;test&quot;</span>, img_array, <span class="number">1</span>, dataformats=<span class="string">&#x27;HWC&#x27;</span>)  <span class="comment"># 添加图像到日志文件中 dataformats=&#x27;HWC&#x27;表示数据格式为高度、宽度、通道数</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    writer.add_scalar(<span class="string">&quot;y=x&quot;</span>, i, i)  <span class="comment"># 添加标量数据 第一个参数是标签，第二个参数是数据，第三个参数是迭代次数 参数标签手动输入</span></span><br><span class="line"></span><br><span class="line">writer.close()  <span class="comment"># 关闭SummaryWriter对象</span></span><br></pre></td></tr></table></figure></li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>Tips of Python</title>
    <url>/2024/11/01/tips_python/</url>
    <content><![CDATA[<h2 id="数据容器"><a href="#数据容器" class="headerlink" title="数据容器"></a>数据容器</h2><h3 id="列表"><a href="#列表" class="headerlink" title="列表"></a>列表</h3><ol>
<li>常见方法</li>
</ol>
<ul>
<li>list.append(元素)：向列表中追加一个元素</li>
<li>list.extand(容器)：将数据容器的内容依次取出，追加到列表的尾部</li>
<li>list.insert(下标，元素)：在指定下标处，插入指定的元素</li>
<li>del列表[下标]：删除列表指定下标元素</li>
<li>列表.pop(下标)：删除列表指定下标元素</li>
<li>列表.remove(元素)：从前向后，删除此元素第一个匹配项</li>
<li>列表.clear()：清空列表</li>
<li>列表.count(元素)：统计此元素在列表中出现的次数</li>
<li>index(元素)：查找指定元素在列表的下标列表.找不到报错ValueError</li>
<li>len(列表)：统计容器内有多少元素</li>
</ul>
<ol start="2">
<li>特点</li>
</ol>
<ul>
<li>可以容纳多个元素(上限为2**63-1、9223372036854775807个)</li>
<li>可以容纳不同类型的元素（混装）</li>
<li>数据是有序存储的（有下标序号）</li>
<li>允许重复数据存在</li>
<li>可以修改（增加或删除元素等）</li>
</ul>
<h3 id="元组"><a href="#元组" class="headerlink" title="元组"></a>元组</h3><ol>
<li>操作方法</li>
</ol>
<ul>
<li>index()：查找某个数据，如果数据存在返回对应的下标，否则报错 </li>
<li>count()：统计某个数据在当前元组出现的次数 </li>
<li>len(元组)：统计元组内的元素个数</li>
</ul>
<ol start="2">
<li>特点</li>
</ol>
<ul>
<li>不可修改内容(可以修改内部st的内部元素)</li>
<li>有序、任意数量元素、允许重复元素，不可修改</li>
<li>支持for循环</li>
</ul>
<h3 id="字符串"><a href="#字符串" class="headerlink" title="字符串"></a>字符串</h3><ol>
<li>常见方法</li>
</ol>
<ul>
<li>字符串[下标]：根据下标索引取出特定位置字符</li>
<li>字符串.index(字符串)：查找给定字符的第一个匹配项的下标</li>
<li>字符串.replace(字符串1，字符串2)：将字符串内的全部字符串1，替换为字符串符串2，不会修改原字符串，而是得到一个新的</li>
<li>字符串.split(字符串)：按照给定字符串，对字符串进行分隔字符串.不会修改原字符串，而是得到一个新的列表</li>
<li>字符串.strip(字符串)移除首尾的空格和换行符或指定字符串</li>
<li>字符串.count(字符串)：统计字符串内某字符串的出现次数 </li>
<li>len(字符串)：统计字符串的字符个数</li>
</ul>
<ol start="2">
<li>特点</li>
</ol>
<ul>
<li>只可以存储字符串</li>
<li>长度任意(取决于内存大小)</li>
<li>支持下标索引</li>
<li>允许重复字符串存在</li>
<li>不可以修改(增加或者删除元素)</li>
<li>支持for循环</li>
</ul>
<h3 id="序列"><a href="#序列" class="headerlink" title="序列"></a>序列</h3><ol>
<li>定义：内容连续、有序、支持下标索引的一类数据容器</li>
<li>分类：列表、元组、字符串</li>
<li>序列如何切片：序列[起始:结束:步长]</li>
</ol>
<h3 id="集合"><a href="#集合" class="headerlink" title="集合"></a>集合</h3><ol>
<li>常用方法</li>
</ol>
<ul>
<li>集合.add(元素)：集合内添加一个元素</li>
<li>集合.remove(元素)：移除集合内指定的元素</li>
<li>集合.pop()：从集合中随机取出一个元素</li>
<li>集合.clear()：将集合清空</li>
<li>集合1.difference(集合2)得到一个新集合，内含2个集合的差集原有的2个集合内容不变在集合1中，删除集合2中存在的元素</li>
<li>集合1.difference_update(集合2)集合1被修改，集合2不变</li>
<li>集合1.union(集合2)得到1个新集合，内含2个集合的全部元素原有的2个集合内容不变 </li>
<li>len(集合)免add得到一个整数，记录了集合的元素数量</li>
</ul>
<ol start="2">
<li>特点</li>
</ol>
<ul>
<li>可以容纳多个数据</li>
<li>可以容纳不同类型的数据(混装)</li>
<li>数据是无序存储的(不支持下标索引)</li>
<li>不允许重复数据存在</li>
<li>可以修改(增加或删除元素等)</li>
<li>支持for循环</li>
</ul>
<h3 id="字典"><a href="#字典" class="headerlink" title="字典"></a>字典</h3><ol>
<li>常用方法</li>
</ol>
<ul>
<li>字典[Key]：获取指定Key对应的Value值</li>
<li>字典[Key]&#x3D;Value：添加或更新键值对</li>
<li>字典.pop(Key)：取出Key对应的Value并在字典内删除此Key的键值对</li>
<li>字典.clear()：清空字典</li>
<li>字典.keys()：获取字典的全部Key,可用于for循环遍历字典 </li>
<li>len(字典)：计算字典内的元素数量</li>
</ul>
<ol start="2">
<li>特点</li>
</ol>
<ul>
<li>可以容纳多个数据</li>
<li>可以容纳不同类型的数据</li>
<li>每一份数据是KeyValue键值对</li>
<li>可以通过Key获取到Value,Key不可重复（重复会覆盖）</li>
<li>不支持下标索引</li>
<li>可以修改（增加或删除更新元素等）</li>
<li>支持for循环，不支持while循环</li>
</ul>
<h3 id="数据容器的总结"><a href="#数据容器的总结" class="headerlink" title="数据容器的总结"></a>数据容器的总结</h3><ol>
<li>应用场景</li>
</ol>
<ul>
<li>列表：一批数据，可修改、可重复的存储场景</li>
<li>元组：一批数据，不可修改、可重复的存储场景</li>
<li>字符串：一串字符串的存储场景</li>
<li>集合：一批数据，去重存储场景</li>
<li>字典：一批数据，可用Key检索Value的存储场景</li>
</ul>
<ol start="2">
<li>通用功能</li>
</ol>
<ul>
<li>通用for循环：遍历容器(字典是遍历key) </li>
<li>max：容器内最大元素 </li>
<li>min()：容器内最小元素 </li>
<li>len()：容器元素个数 </li>
<li>list()：转换为列表 </li>
<li>tuple()：转换为元组 </li>
<li>str()：转换为字符串 </li>
<li>set()：转换为集合 </li>
<li>sorted(序列，[reverse&#x3D;True])：排序，reverse:&#x3D;True表示降序得到一个排好序的列表</li>
</ul>
<h2 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h2><h3 id="参数传递"><a href="#参数传递" class="headerlink" title="参数传递"></a>参数传递</h3><ol>
<li>掌握位置参数</li>
</ol>
<ul>
<li>根据参数位置来传递参数</li>
</ul>
<ol start="2">
<li>掌握关键字参数</li>
</ol>
<ul>
<li>通过“键&#x3D;值”形式传递参数，可以不限参数顺序</li>
<li>可以和位置参数混用，位置参数需在前</li>
</ul>
<ol start="3">
<li>掌握缺省参数</li>
</ol>
<ul>
<li>不传递参数值时会使用默认的参数值</li>
<li>默认值的参数必须定义在最后</li>
</ul>
<ol start="4">
<li>掌握不定长参数</li>
</ol>
<ul>
<li>位置不定长传递以*号标记一个形式参数，以元组的形式接受参数，形式参数一般命名为args</li>
<li>关键字不定长传递以**号标记一个形式参数，以字典的形式接受参数，形式参数一般命名为kwargs</li>
</ul>
<h3 id="lambda匿名函数"><a href="#lambda匿名函数" class="headerlink" title="lambda匿名函数"></a>lambda匿名函数</h3><ol>
<li>匿名函数使用lambda关键字进行定义 </li>
<li>定义语法：lambda传入参数：函数体（一行代码） </li>
<li>注意事项：</li>
</ol>
<ul>
<li>匿名函数用于临时构建一个函数只用一次的场景</li>
<li>匿名函数的定义中，函数体只能写一行代码，如果函数体要写多行代码，不可用lambda匿名函数，应使用def定义带名函数</li>
</ul>
<h2 id="文件"><a href="#文件" class="headerlink" title="文件"></a>文件</h2><ol>
<li>读入操作汇总</li>
</ol>
<ul>
<li>文件对象open(file,mode,encoding)：打开文件获得文件对象</li>
<li>文件对象.read(num)：读取指定长度字节不指定num读取文件全部</li>
<li>文件对象.readline()：读取一行</li>
<li>文件对象.readlines()：读取全部行，得到列表 </li>
<li>for line in 文件对象：fo循环文件行，一次循环得到一行数据</li>
<li>文件对象.close()：关闭文件对象 </li>
<li>with open()as f：通过with openi语法打开文件，可以自动关闭</li>
</ul>
<ol start="2">
<li>写入操作汇总</li>
</ol>
<ul>
<li>写入文件使用open函数的” W模式进行写入 </li>
<li>写入的方法有：<ul>
<li>wirte(),写入内容</li>
<li>flush(),刷新内容到硬盘中</li>
</ul>
</li>
<li>注意事项：<ul>
<li>W模式，文件不存在，会创建新文件 </li>
<li>W模式，文件存在，会清空原有内容</li>
<li>close()方法，带有flush()方法的功能</li>
</ul>
</li>
</ul>
<h2 id="异常"><a href="#异常" class="headerlink" title="异常"></a>异常</h2><ol>
<li>为什么要捕获异常<br>在可能发生异常的地方，进行捕获。当异常出现的时候，提供解决方式，而不是任由其导致程序无法运行。 </li>
<li>捕获异常的语法<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    可能要发生异常的语句 </span><br><span class="line"><span class="keyword">except</span>[异常 <span class="keyword">as</span> 别名:]</span><br><span class="line">    出现异常的准备手段</span><br><span class="line">    [<span class="keyword">else</span>]:</span><br><span class="line">    未出现异常时应做的事情</span><br><span class="line">    [<span class="keyword">finally</span>]:</span><br><span class="line">    不管出不出现异常都会做的事情 </span><br></pre></td></tr></table></figure></li>
<li>如何捕获所有异常</li>
</ol>
<ul>
<li>异常的种类多种多样，如果想要不管什么类型的异常都能捕获到，那么使用：<ul>
<li>except: </li>
<li>except Exception:</li>
</ul>
</li>
</ul>
<h2 id="模块"><a href="#模块" class="headerlink" title="模块"></a>模块</h2><ol>
<li>如何自定义模块并导入<br>在Python代码文件中正常写代码即可，通过import、from关键字和导入Python内置模块一样导入即可使用。 </li>
<li>main变量的功能是<br>if_main_&#x3D;”<em>main</em>“表示，只有当程序是直接执行的才会进入 if内部，如果是被导入的，则if无法进入 </li>
<li>注意事项<ul>
<li>不同模块，同名的功能，如果都被导入，那么后导入的会覆盖先导入的中 </li>
<li>__all_变量可以控制import *的时候哪些功能可以被导入</li>
</ul>
</li>
</ol>
<h2 id="json数据格式转换"><a href="#json数据格式转换" class="headerlink" title="json数据格式转换"></a>json数据格式转换</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> json  <span class="comment">#引入json包</span></span><br><span class="line">data = []  <span class="comment">#可以是字典或者是列表</span></span><br><span class="line">json.dumps(data，ensure_ascii=<span class="literal">False</span>)  <span class="comment">#转换成字符串类型</span></span><br><span class="line">json.loads(data)  <span class="comment">#将json数据类型还原成列表类型文件</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="pyecharts模块折线图可视化"><a href="#pyecharts模块折线图可视化" class="headerlink" title="pyecharts模块折线图可视化"></a>pyecharts模块折线图可视化</h2><ul>
<li>想要做出数据可视化效果图，可以借助pyecharts模块来完成</li>
<li><a href="https://05x-docs.pyecharts.org/#/">pyecharts 文档</a></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyecharts.charts <span class="keyword">import</span> Line</span><br><span class="line"><span class="keyword">from</span> pyecharts <span class="keyword">import</span> options <span class="keyword">as</span> opts</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个折线图对象</span></span><br><span class="line">line = Line()</span><br><span class="line"><span class="comment"># 给折线图对象添X抽的数据</span></span><br><span class="line">Line.add_xaxis([<span class="string">&quot;中国&quot;</span>, <span class="string">&quot;美国&quot;</span>, <span class="string">&quot;英国&quot;</span>])</span><br><span class="line"><span class="comment"># 给折线图对象添加则轴的数据</span></span><br><span class="line">line.add_yaxis(<span class="string">&quot;GDP&quot;</span>, [<span class="number">30</span>, <span class="number">20</span>, <span class="number">10</span>])</span><br><span class="line"><span class="comment"># 给折线图对象添加标题</span></span><br><span class="line">line.set_global_opts(</span><br><span class="line">    title_opts=opts.TitleOpts(title=<span class="string">&quot;GDP展示&quot;</span>, pos_left=<span class="string">&quot;center&quot;</span>, pos_bottom=<span class="string">&quot;bottom&quot;</span>),  <span class="comment"># 标题</span></span><br><span class="line">    legend_opts=opts.LegendOpts(is_show=<span class="literal">True</span>),  <span class="comment"># 图例</span></span><br><span class="line">    tooltip_opts=opts.TooltipOpts(is_show=<span class="literal">True</span>),  <span class="comment"># 提示框</span></span><br><span class="line">    toolbox_opts=opts.ToolboxOpts(is_show=<span class="literal">True</span>),  <span class="comment"># 工具箱</span></span><br><span class="line">    visualmap_opts=opts.VisualMapOpts(is_show=<span class="literal">True</span>, max_=<span class="number">30</span>, min_=<span class="number">10</span>)  <span class="comment"># 视觉映射</span></span><br><span class="line">)</span><br><span class="line"><span class="comment"># 通过render方法，将代码生成为图像</span></span><br><span class="line">line.render()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="pyecharts的地图可视化"><a href="#pyecharts的地图可视化" class="headerlink" title="pyecharts的地图可视化"></a>pyecharts的地图可视化</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyecharts.charts <span class="keyword">import</span> Map</span><br><span class="line"><span class="keyword">from</span> pyecharts.options <span class="keyword">import</span> VisualMapOpts</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成地图</span></span><br><span class="line"><span class="built_in">map</span> = Map()</span><br><span class="line"><span class="comment"># 设置地图的标题和数据</span></span><br><span class="line">data = [(<span class="string">&quot;广东&quot;</span>, <span class="number">104</span>), (<span class="string">&quot;北京&quot;</span>, <span class="number">60</span>), (<span class="string">&quot;上海&quot;</span>, <span class="number">50</span>), (<span class="string">&quot;江苏&quot;</span>, <span class="number">30</span>), (<span class="string">&quot;浙江&quot;</span>, <span class="number">20</span>)]</span><br><span class="line"><span class="comment"># 添加数据</span></span><br><span class="line"><span class="built_in">map</span>.add(<span class="string">&quot;中国地图&quot;</span>, data, <span class="string">&quot;china&quot;</span>)</span><br><span class="line"><span class="comment"># 设置地图的标题和颜色</span></span><br><span class="line"><span class="built_in">map</span>.set_global_opts(</span><br><span class="line">    visualmap_opts=VisualMapOpts(</span><br><span class="line">        is_show=<span class="literal">True</span>,  <span class="comment"># 是否显示</span></span><br><span class="line">        is_piecewise=<span class="literal">True</span>,  <span class="comment"># 是否分段显示 手动设置</span></span><br><span class="line">        pieces=[  <span class="comment"># 设置分段的颜色</span></span><br><span class="line">            &#123;<span class="string">&quot;min&quot;</span>: <span class="number">0</span>, <span class="string">&quot;max&quot;</span>: <span class="number">50</span>, <span class="string">&quot;label&quot;</span>: <span class="string">&quot;0-50&quot;</span>, <span class="string">&quot;color&quot;</span>: <span class="string">&quot;#50a3ba&quot;</span>&#125;,</span><br><span class="line">            &#123;<span class="string">&quot;min&quot;</span>: <span class="number">51</span>, <span class="string">&quot;max&quot;</span>: <span class="number">100</span>, <span class="string">&quot;label&quot;</span>: <span class="string">&quot;51-100&quot;</span>, <span class="string">&quot;color&quot;</span>: <span class="string">&quot;#e098c7&quot;</span>&#125;,</span><br><span class="line">            &#123;<span class="string">&quot;min&quot;</span>: <span class="number">101</span>, <span class="string">&quot;max&quot;</span>: <span class="number">150</span>, <span class="string">&quot;label&quot;</span>: <span class="string">&quot;101-150&quot;</span>, <span class="string">&quot;color&quot;</span>: <span class="string">&quot;#f6e8c3&quot;</span>&#125;,</span><br><span class="line">            &#123;<span class="string">&quot;min&quot;</span>: <span class="number">151</span>, <span class="string">&quot;max&quot;</span>: <span class="number">200</span>, <span class="string">&quot;label&quot;</span>: <span class="string">&quot;151-200&quot;</span>, <span class="string">&quot;color&quot;</span>: <span class="string">&quot;#f2c5a4&quot;</span>&#125;,</span><br><span class="line">            &#123;<span class="string">&quot;min&quot;</span>: <span class="number">201</span>, <span class="string">&quot;max&quot;</span>: <span class="number">300</span>, <span class="string">&quot;label&quot;</span>: <span class="string">&quot;201-300&quot;</span>, <span class="string">&quot;color&quot;</span>: <span class="string">&quot;#e0631a&quot;</span>&#125;,</span><br><span class="line">            &#123;<span class="string">&quot;min&quot;</span>: <span class="number">301</span>, <span class="string">&quot;max&quot;</span>: <span class="number">400</span>, <span class="string">&quot;label&quot;</span>: <span class="string">&quot;301-400&quot;</span>, <span class="string">&quot;color&quot;</span>: <span class="string">&quot;#b53b3a&quot;</span>&#125;,</span><br><span class="line">            &#123;<span class="string">&quot;min&quot;</span>: <span class="number">401</span>, <span class="string">&quot;max&quot;</span>: <span class="number">500</span>, <span class="string">&quot;label&quot;</span>: <span class="string">&quot;401-500&quot;</span>, <span class="string">&quot;color&quot;</span>: <span class="string">&quot;#b70f0a&quot;</span>&#125;,</span><br><span class="line">        ],</span><br><span class="line">    )</span><br><span class="line">)</span><br><span class="line"><span class="comment"># 绘制地图</span></span><br><span class="line"><span class="built_in">map</span>.render(<span class="string">&quot;basiclearning.html&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>


]]></content>
  </entry>
  <entry>
    <title>Objects</title>
    <url>/2024/11/06/objects/</url>
    <content><![CDATA[<h2 id="类的成员方法"><a href="#类的成员方法" class="headerlink" title="类的成员方法"></a>类的成员方法</h2><ol>
<li>类是由哪两部分组成呢</li>
</ol>
<ul>
<li>类的属性，称之为：成员变量</li>
<li>类的行为，称之为：成员方法<br>注意：函数是写在类外的，定义在类内部，我们都称之为方法哦</li>
</ul>
<ol start="2">
<li>类和成员方法的定义语法<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">c1ass 类名称:</span><br><span class="line">    成员变量 </span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">成员方法</span>(<span class="params">se1f,参数列表</span>):</span><br><span class="line">        成员方法体</span><br><span class="line"></span><br><span class="line">对象 = 类名称()</span><br></pre></td></tr></table></figure></li>
<li>self的作用</li>
</ol>
<ul>
<li>表示类对象本身的意思</li>
<li>只有通过self,成员方法才能访问类的成员变量 </li>
<li>self出现在形参列表中，但是不占用参数位置，无需理会</li>
</ul>
<h2 id="构造方法-魔术方法"><a href="#构造方法-魔术方法" class="headerlink" title="构造方法&amp;魔术方法"></a>构造方法&amp;魔术方法</h2><ol>
<li>构造方法的名称是：<br>__init__注意init前后的2个下划线符号 </li>
<li>构造方法的作用：</li>
</ol>
<ul>
<li>构建类对象的时候会自动运行</li>
<li>构建类对象的传参会传递给构造方法，借此特性可以给成员变量赋值</li>
</ul>
<ol start="3">
<li>注意事项：</li>
</ol>
<ul>
<li>构造方法不要忘记self关键字</li>
<li>在方法内使用成员变量需要使用self</li>
</ul>
<hr>
<ol>
<li>__init__：构造方法，可用于创建类对象的时候设置初始化行为</li>
<li>__str__：用于实现类对象转字符串的行为</li>
<li>__lt__：用于2个类对象进行小于或大于比较</li>
<li>__le__：用于2个类对象进行小于等于或大于等于比较</li>
<li>__eq__：用于2个类对象进行相等比较</li>
</ol>
<h2 id="封装"><a href="#封装" class="headerlink" title="封装"></a>封装</h2><ol>
<li>封装的概念是指<br>将现实世界事物在类中描述为属性和方法，即为封装。 </li>
<li>什么是私有成员、为什么需要私有成员<br>现实事物有部分属性和行为是不公开对使用者开放的。同样在类中描属性和方法的时候也需要达到这个要求，就需要定义私有成员了 </li>
<li>如何定义私有成员<br>成员变量和成员方法的命名均以__作为开头即可 </li>
<li>私有成员的访问限制</li>
</ol>
<ul>
<li>类对象无法访问私有成员</li>
<li>类中的其它成员可以访问私有成员</li>
</ul>
<h2 id="继承"><a href="#继承" class="headerlink" title="继承"></a>继承</h2><ol>
<li>什么是继承<br>继承就是一个类，继承另外一个类的成员变量和成员方法<br>语法： <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">c1ass 类(父类[，父类<span class="number">2</span>，，....，父类N]):</span><br><span class="line">    类内容体</span><br></pre></td></tr></table></figure>
子类构建的类对象，可以</li>
</ol>
<ul>
<li>有自己的成员变量和成员方法</li>
<li>使用父类的成员变量和成员方法</li>
</ul>
<ol start="2">
<li>单继承和多继承</li>
</ol>
<ul>
<li>单继承：一个类继承另一个类</li>
<li>多继承：一个类继承多个类，按照顺序从左向右依次继承</li>
<li>多继承中，如果父类有同名方法或属性，先继承的优先级高于后继承</li>
</ul>
<ol start="3">
<li><p>pass关键字的作用是什么<br>pass是占位语句，用来保证函数（方法）或类定义的完整性，表示无内容，空的意思</p>
</li>
<li><p>复写表示：<br>对父类的成员属性或成员方法进行重新定义 </p>
</li>
<li><p>复写的语法：<br>在子类中重新实现同名成员方法或成员属性即可 </p>
</li>
<li><p>在子类中，如何调用父类成员</p>
</li>
</ol>
<ul>
<li>方式1：调用父类成员<ul>
<li>使用成员变量：父类名.成员变量</li>
<li>使用成员方法：父类名.成员方法(self)</li>
</ul>
</li>
<li>方式2：使用super()调用父类成员<ul>
<li>使用成员变量：super().成员变量</li>
<li>使用成员方法：super().成员方法</li>
</ul>
</li>
<li>注意：只可以在子类内部调用父类的同名成员，子类的实体类对象调用默认是调用子类复写的</li>
</ul>
<h2 id="类型注解"><a href="#类型注解" class="headerlink" title="类型注解"></a>类型注解</h2><h3 id="变量的类型注解"><a href="#变量的类型注解" class="headerlink" title="变量的类型注解"></a>变量的类型注解</h3><ol>
<li>变量：类型语法</li>
<li>在注释中，#type:类型</li>
</ol>
<h3 id="函数和方法类型注解"><a href="#函数和方法类型注解" class="headerlink" title="函数和方法类型注解"></a>函数和方法类型注解</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">函数方法名</span>(<span class="params">形参：类型，.，..·.，形参：类型</span>)-&gt;返回值类型:</span><br><span class="line">     <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>

<h3 id="Union联合类型注解"><a href="#Union联合类型注解" class="headerlink" title="Union联合类型注解"></a>Union联合类型注解</h3><ol>
<li>导包：from typing import Union</li>
<li>使用：Union[类型，……，类型]</li>
</ol>
<h2 id="多态"><a href="#多态" class="headerlink" title="多态"></a>多态</h2>]]></content>
  </entry>
  <entry>
    <title>Transformer</title>
    <url>/2024/10/30/transformer/</url>
    <content><![CDATA[<h1 id="Transformer模型详解"><a href="#Transformer模型详解" class="headerlink" title="Transformer模型详解"></a>Transformer模型详解</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>Transformer由论文 <strong>《Attention is All You Need》</strong> 提出，现在是谷歌云TPU推荐的参考模型。论文相关的Tensorflow的代码可以从GitHub获取，其作为Tensor2Tensor包的一部分。哈佛的NLP团队也实现了一个基于PyTorch的版本，并注释该论文。<br>在本文中，我们将试图把模型简化一点，并逐一介绍里面的核心概念，希望让普通读者也能轻易理解。</p>
<h2 id="Transformer-整体结构"><a href="#Transformer-整体结构" class="headerlink" title="Transformer 整体结构"></a>Transformer 整体结构</h2><p>首先介绍 Transformer 的整体结构，下图是 Transformer 用于中英文翻译的整体结构：</p>
<p><img src="https://s21.ax1x.com/2024/10/25/pAwslg1.png" alt="pAwslg1.png"></p>
<p>可以看到 Transformer 由 Encoder 和 Decoder 两个部分组成，Encoder 和 Decoder 都包含 6 个 block。Transformer 的工作流程大体如下：<br>第一步获取输入句子的每一个单词的表示向量 X，X由单词的 Embedding（Embedding就是从原始数据提取出来的Feature） 和单词位置的 Embedding 相加得到。<br><img src="https://s21.ax1x.com/2024/10/30/pAB0eSI.png" alt="pAB0eSI.png"></p>
<p>第二步将得到的单词表示向量矩阵 (如上图所示，每一行是一个单词的表示 x) 传入 Encoder 中，经过 6 个 Encoder block 后可以得到句子所有单词的编码信息矩阵 C，如下图。单词向量矩阵用  表示， n 是句子中单词个数，d 是表示向量的维度 (论文中 d&#x3D;512)。每一个 Encoder block 输出的矩阵维度与输入完全一致。<br><img src="https://s21.ax1x.com/2024/10/30/pAB0Mm8.png" alt="pAB0Mm8.png"></p>
<p>第三步：将 Encoder 输出的编码信息矩阵 C传递到 Decoder 中，Decoder 依次会根据当前翻译过的单词 1~ i 翻译下一个单词 i+1，如下图所示。在使用的过程中，翻译到单词 i+1 的时候需要通过 Mask (掩盖) 操作遮盖住 i+1 之后的单词。<br><img src="https://s21.ax1x.com/2024/10/30/pAB08Yj.png" alt="pAB08Yj.png"></p>
<p>上图 Decoder 接收了 Encoder 的编码矩阵 C，然后首先输入一个翻译开始符 “”，预测第一个单词 “I”；然后输入翻译开始符 “” 和单词 “I”，预测单词 “have”，以此类推。这是 Transformer 使用时候的大致流程，接下来是里面各个部分的细节。</p>
<h2 id="Transformer-的输入"><a href="#Transformer-的输入" class="headerlink" title="Transformer 的输入"></a>Transformer 的输入</h2><p>Transformer 中单词的输入表示 x由单词 Embedding 和位置 Embedding （Positional Encoding）相加得到。<br><img src="https://s21.ax1x.com/2024/10/30/pAB0tlq.png" alt="pAB0tlq.png"></p>
<h3 id="Word-Embedding"><a href="#Word-Embedding" class="headerlink" title="Word Embedding"></a>Word Embedding</h3><p>单词的 Embedding 有很多种方式可以获取，例如可以采用 Word2Vec、Glove 等算法预训练得到，也可以在 Transformer 中训练得到。</p>
<h3 id="Positional-Embedding"><a href="#Positional-Embedding" class="headerlink" title="Positional Embedding"></a>Positional Embedding</h3><p>Transformer 中除了单词的 Embedding，还需要使用Positional Embedding 表示单词出现在句子中的位置。 <strong>因为 Transformer 不采用 RNN 的结构，而是使用全局信息，不能利用单词的顺序信息，而这部分信息对于 NLP 来说非常重要。</strong> 所以 Transformer 中使用Positional Embedding 保存单词在序列中的相对或绝对位置。</p>
<p>Positional Embedding 用 PE表示，PE 的维度与单词 Embedding 是一样的。PE 可以通过训练得到，也可以使用某种公式计算得到。在 Transformer 中采用了后者，计算公式如下：<br><img src="https://s21.ax1x.com/2024/10/25/pAwsWCj.png" alt="pAwsWCj.png"></p>
<p>其中，pos 表示单词在句子中的位置，d 表示 PE的维度 (与词 Embedding 一样)，2i 表示偶数的维度，2i+1 表示奇数维度 (即 2i≤d, 2i+1≤d)。使用这种公式计算 PE 有以下的好处：</p>
<ul>
<li>使 PE 能够适应比训练集里面所有句子更长的句子，假设训练集里面最长的句子是有 20 个单词，突然来了一个长度为 21 的句子，则使用公式计算的方法可以计算出第 21 位的 Embedding。</li>
<li>可以让模型容易地计算出相对位置，对于固定长度的间距 k，<strong>PE(pos+k)</strong> 可以用 <strong>PE(pos)</strong> 计算得到。因为 Sin(A+B) &#x3D; Sin(A)Cos(B) + Cos(A)Sin(B), Cos(A+B) &#x3D; Cos(A)Cos(B) - Sin(A)Sin(B)。</li>
</ul>
<p>将单词的Word Embedding 和Positional Embedding 相加，就可以得到单词的表示向量 x，x 就是 Transformer 的输入。</p>
<h2 id="Self-Attention（自注意力机制）"><a href="#Self-Attention（自注意力机制）" class="headerlink" title="Self-Attention（自注意力机制）"></a>Self-Attention（自注意力机制）</h2><p><img src="https://s21.ax1x.com/2024/10/30/pAB5XDg.png" alt="pAB5XDg.png"></p>
<p>上图是论文中 Transformer 的内部结构图，左侧为 Encoder block，右侧为 Decoder block。红色圈中的部分为 Multi-Head Attention，是由多个 Self-Attention组成的，可以看到 Encoder block 包含一个 Multi-Head Attention，而 Decoder block 包含两个 Multi-Head Attention (其中有一个用到 Masked)。Multi-Head Attention 上方还包括一个 Add &amp; Norm 层，Add 表示残差连接 (Residual Connection) 用于防止网络退化，Norm 表示 Layer Normalization，用于对每一层的激活值进行归一化。</p>
<p>因为 Self-Attention是 Transformer 的重点，所以我们重点关注 Multi-Head Attention 以及 Self-Attention，首先详细了解一下 Self-Attention 的内部逻辑。</p>
<h3 id="Self-Attention-结构"><a href="#Self-Attention-结构" class="headerlink" title="Self-Attention 结构"></a>Self-Attention 结构</h3><p><img src="https://s21.ax1x.com/2024/10/30/pAB5xEj.png" alt="pAB5xEj.png"></p>
<p>上图是 Self-Attention 的结构，在计算的时候需要用到矩阵Q(查询),K(键值),V(值)。在实际中，Self-Attention 接收的是输入(单词的表示向量x组成的矩阵X) 或者上一个 Encoder block 的输出。而Q,K,V正是通过 Self-Attention 的输入进行线性变换得到的。</p>
<h3 id="Q-K-V-的计算"><a href="#Q-K-V-的计算" class="headerlink" title="Q, K, V 的计算"></a>Q, K, V 的计算</h3><p>Self-Attention 的输入用矩阵X进行表示，则可以使用线性变阵矩阵<strong>WQ,WK,WV</strong>计算得到<strong>Q,K,V</strong>。计算如下图所示，<strong>注意 X, Q, K, V 的每一行都表示一个单词</strong>。<br><img src="https://s21.ax1x.com/2024/10/30/pABI9Cq.png" alt="pABI9Cq.png"></p>
<h3 id="Self-Attention-的输出"><a href="#Self-Attention-的输出" class="headerlink" title="Self-Attention 的输出"></a>Self-Attention 的输出</h3><p>得到矩阵 Q, K, V之后就可以计算出 Self-Attention 的输出了，计算的公式如下：<br><img src="https://s21.ax1x.com/2024/10/30/pABIP2V.png" alt="pABIP2V.png"></p>
<p>公式中计算矩阵Q和K每一行向量的内积，为了防止内积过大，因此除以$d_k$  的平方根。Q乘以K的转置后，得到的矩阵行列数都为 n，n 为句子单词数，这个矩阵可以表示单词之间的 attention 强度。下图为Q乘以 $K^T$ ，1234 表示的是句子中的单词。<br><img src="https://s21.ax1x.com/2024/10/30/pABIivT.png" alt="pABIivT.png"></p>
<p>得到Q$K^T$ 之后，使用 Softmax 计算每一个单词对于其他单词的 attention 系数，公式中的 Softmax 是对矩阵的每一行进行 Softmax，即每一行的和都变为 1.<br><img src="https://s21.ax1x.com/2024/10/30/pABIkKU.png" alt="pABIkKU.png"></p>
<p>得到 Softmax 矩阵之后可以和V相乘，得到最终的输出Z。<br><img src="https://s21.ax1x.com/2024/10/30/pABIEb4.png" alt="pABIEb4.png"></p>
<p>上图中 Softmax 矩阵的第 1 行表示单词 1 与其他所有单词的 attention 系数，最终单词 1 的输出 $Z_1$ 等于所有单词 i 的值得到 8 个输出矩阵  到  之后，Multi-Head Attention 将它们拼接在一起 (Concat)，然后传入一个Linear层，得到 Multi-Head Attention 最终的输出Z。 根据 attention 系数的比例加在一起得到，如下图所示：<br><img src="https://s2.loli.net/2024/10/30/zj2QU14DLwmgRfs.png" alt="image.png"></p>
<h3 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h3><p>在上一步，我们已经知道怎么通过 Self-Attention 计算得到输出矩阵 Z，而 Multi-Head Attention 是由多个 Self-Attention 组合形成的，下图是论文中 Multi-Head Attention 的结构图。<br><img src="https://s2.loli.net/2024/10/30/XNrobGZMkgOeB57.png" alt="image.png"></p>
<p>从上图可以看到 Multi-Head Attention 包含多个 Self-Attention 层，首先将输入X分别传递到 h 个不同的 Self-Attention 中，计算得到 h 个输出矩阵Z。下图是 h&#x3D;8 时候的情况，此时会得到 8 个输出矩阵Z。<br><img src="https://s2.loli.net/2024/10/30/W8hdnBT5bDufAcC.png" alt="image.png"></p>
<p>得到 8 个输出矩阵$Z_1$  到 $Z_8$ 之后，Multi-Head Attention 将它们拼接在一起 (Concat)，然后传入一个Linear层，得到 Multi-Head Attention 最终的输出Z。<br><img src="https://s2.loli.net/2024/10/30/8XiUxMfk7pomsh6.png" alt="image.png"></p>
<p>可以看到 Multi-Head Attention 输出的矩阵Z与其输入的矩阵X的维度是一样的。</p>
<h2 id="Encoder-结构"><a href="#Encoder-结构" class="headerlink" title="Encoder 结构"></a>Encoder 结构</h2><p><img src="https://s2.loli.net/2024/10/30/Lpc4W7BTfHMbJmo.png" alt="image.png"></p>
<p>上图红色部分是 Transformer 的 Encoder block 结构，可以看到是由 <strong>Multi-Head Attention, Add &amp; Norm, Feed Forward, Add &amp; Norm</strong> 组成的。刚刚已经了解了 Multi-Head Attention 的计算过程，现在了解一下 Add &amp; Norm 和 Feed Forward 部分。</p>
<h3 id="Add-Norm"><a href="#Add-Norm" class="headerlink" title="Add &amp; Norm"></a>Add &amp; Norm</h3><p>Add &amp; Norm 层由 Add 和 Norm 两部分组成，其计算公式如下：<br><img src="https://s2.loli.net/2024/10/30/x81oylVfXpgPtz6.png" alt="image.png"></p>
<p>其中 X表示 Multi-Head Attention 或者 Feed Forward 的输入，MultiHeadAttention(X) 和 FeedForward(X) 表示输出 (输出与输入 X 维度是一样的，所以可以相加)。<br>Add指 X+MultiHeadAttention(X)，是一种残差连接，通常用于解决多层网络训练的问题，可以让网络只关注当前差异的部分，在 ResNet 中经常用到：<br><img src="https://s2.loli.net/2024/10/30/FJDIuj3PkiVSe9Q.png" alt="image.png"></p>
<p>Norm指 Layer Normalization，通常用于 RNN 结构，Layer Normalization 会将每一层神经元的输入都转成均值方差都一样的，这样可以加快收敛。</p>
<h3 id="Feed-Forward"><a href="#Feed-Forward" class="headerlink" title="Feed Forward"></a>Feed Forward</h3><p>Feed Forward 层比较简单，是一个两层的全连接层，第一层的激活函数为 Relu，第二层不使用激活函数，对应的公式如下。<br><img src="https://s2.loli.net/2024/10/30/AbVZj3eovG6q5pB.png" alt="image.png"></p>
<p>X是输入，Feed Forward 最终得到的输出矩阵的维度与X一致。</p>
<h3 id="组成-Encoder"><a href="#组成-Encoder" class="headerlink" title="组成 Encoder"></a>组成 Encoder</h3><p>通过上面描述的 Multi-Head Attention, Fed Forward, Add &amp; Norm 就可以造出一个 Encoder block，Encoder block 接收输入矩阵 $X_{(n×d)}$  ，并输出一个矩阵 $O_{（n×d）}$ 。通过多个 Encoder block 叠加就可以组成 Encoder。</p>
<p>第一个 Encoder block 的输入为句子单词的表示向量矩阵，后续 Encoder block 的输入是前一个 Encoder block 的输出，最后一个 Encoder block 输出的矩阵就是<strong>编码信息矩阵 C</strong>，这一矩阵后续会用到 Decoder 中。</p>
<p><img src="https://s2.loli.net/2024/10/30/JiebPr4VuzdHqYL.png" alt="image.png"></p>
<h2 id="Decoder-结构"><a href="#Decoder-结构" class="headerlink" title="Decoder 结构"></a>Decoder 结构</h2><p><img src="https://s2.loli.net/2024/10/30/PsgNlJBQ3Eh1AX2.png" alt="image.png"></p>
<p>上图红色部分为 Transformer 的 Decoder block 结构，与 Encoder block 相似，但是存在一些区别：</p>
<ul>
<li>包含两个 Multi-Head Attention 层。</li>
<li>第一个 Multi-Head Attention 层采用了 Masked 操作。</li>
<li>第二个 Multi-Head Attention 层的K, V矩阵使用 Encoder 的编码信息矩阵C进行计算，而Q使用上一个 Decoder block 的输出计算。</li>
<li>最后有一个 Softmax 层计算下一个翻译单词的概率。</li>
</ul>
<h3 id="第一个-Multi-Head-Attention"><a href="#第一个-Multi-Head-Attention" class="headerlink" title="第一个 Multi-Head Attention"></a>第一个 Multi-Head Attention</h3><p>Decoder block 的第一个 Multi-Head Attention 采用了 Masked 操作，因为在翻译的过程中是顺序翻译的，即翻译完第 i 个单词，才可以翻译第 i+1 个单词。通过 Masked 操作可以防止第 i 个单词知道 i+1 个单词之后的信息。下面以 “我有一只猫” 翻译成 “I have a cat” 为例，了解一下 Masked 操作。</p>
<p>下面的描述中使用了类似 Teacher Forcing 的概念，不熟悉 Teacher Forcing 的童鞋可以参考以下上一篇文章Seq2Seq 模型详解。在 Decoder 的时候，是需要根据之前的翻译，求解当前最有可能的翻译，如下图所示。首先根据输入 “Begin” 预测出第一个单词为 “I”，然后根据输入 “Begin I” 预测下一个单词 “have”。</p>
<p><img src="https://s2.loli.net/2024/10/30/aTIBEvfKjVuOGZU.png" alt="image.png"></p>
<p>Decoder 可以在训练的过程中使用 Teacher Forcing 并且并行化训练，即将正确的单词序列 ( I have a cat) 和对应输出 (I have a cat ) 传递到 Decoder。那么在预测第 i 个输出时，就要将第 i+1 之后的单词掩盖住，<strong>注意 Mask 操作是在 Self-Attention 的 Softmax 之前使用的，下面用 0 1 2 3 4 5 分别表示</strong> “ I have a cat “。</p>
<p>第一步：是 Decoder 的输入矩阵和 <strong>Mask</strong> 矩阵，输入矩阵包含 “ I have a cat” (0, 1, 2, 3, 4) 五个单词的表示向量，<strong>Mask</strong> 是一个 5×5 的矩阵。在 <strong>Mask</strong> 可以发现单词 0 只能使用单词 0 的信息，而单词 1 可以使用单词 0, 1 的信息，即只能使用之前的信息。</p>
<p><img src="https://s2.loli.net/2024/10/30/EiaCLbpUQRrZhS9.png" alt="image.png"></p>
<p>第二步： 接下来的操作和之前的 Self-Attention 一样，通过输入矩阵X计算得到Q,K,V矩阵。然后计算Q和  $K^T$的乘积Q  $K^T$  。<br><img src="https://s2.loli.net/2024/10/30/FWhNjz1ldAkvPyG.png" alt="image.png"></p>
<p>第三步： 在得到  之后需要进行 Softmax，计算 attention score，我们在 Softmax 之前需要使用Mask矩阵遮挡住每一个单词之后的信息，遮挡操作如下：<br><img src="https://s2.loli.net/2024/10/30/Xdi7cNgyRDkxzvW.png" alt="image.png"><br>得到 Mask  Q $K^T$之后在 Mask  Q $K^T$上进行 Softmax，每一行的和都为 1。但是单词 0 在单词 1, 2, 3, 4 上的 attention score 都为 0。</p>
<p>第四步：使用 Mask Q $K^T$与矩阵 V相乘，得到输出 Z，则单词 1 的输出向量 $Z_1$ 是只包含单词 1 信息的。<br><img src="https://s2.loli.net/2024/10/30/2sOiw6lECWVMr1a.png" alt="image.png"></p>
<p>第五步： 通过上述步骤就可以得到一个 Mask Self-Attention 的输出矩阵 $Z_i$  ，然后和 Encoder 类似，通过 Multi-Head Attention 拼接多个输出 $Z_i$ 然后计算得到第一个 Multi-Head Attention 的输出Z，Z与输入X维度一样。</p>
<h3 id="第二个-Multi-Head-Attention"><a href="#第二个-Multi-Head-Attention" class="headerlink" title="第二个 Multi-Head Attention"></a>第二个 Multi-Head Attention</h3><p>Decoder block 第二个 Multi-Head Attention 变化不大， 主要的区别在于其中 Self-Attention 的 K, V矩阵不是使用 上一个 Decoder block 的输出计算的，而是使用 Encoder 的编码信息矩阵 C 计算的。</p>
<p>根据 Encoder 的输出 C计算得到 K, V，根据上一个 Decoder block 的输出 Z 计算 Q (如果是第一个 Decoder block 则使用输入矩阵 X 进行计算)，后续的计算方法与之前描述的一致。</p>
<p>这样做的好处是在 Decoder 的时候，每一位单词都可以利用到 Encoder 所有单词的信息 (这些信息无需 Mask)。</p>
<h3 id="Softmax-预测输出单词"><a href="#Softmax-预测输出单词" class="headerlink" title="Softmax 预测输出单词"></a>Softmax 预测输出单词</h3><p>Decoder block 最后的部分是利用 Softmax 预测下一个单词，在之前的网络层我们可以得到一个最终的输出 Z，因为 Mask 的存在，使得单词 0 的输出 Z0 只包含单词 0 的信息，如下：</p>
<p><img src="https://s2.loli.net/2024/10/30/zOr64NapThSgBjs.png" alt="image.png"></p>
<p>Softmax 根据输出矩阵的每一行预测下一个单词：<br><img src="https://s2.loli.net/2024/10/30/2DjvHPpSxyLgm7z.png" alt="image.png"></p>
<p>这就是 Decoder block 的定义，与 Encoder 一样，Decoder 是由多个 Decoder block 组合而成。</p>
<hr>
<ul>
<li>Transformer 与 RNN 不同，可以比较好地并行训练。</li>
<li>Transformer 本身是不能利用单词的顺序信息的，因此需要在输入中添加位置 Embedding，否则 Transformer 就是一个词袋模型了。</li>
<li>Transformer 的重点是 Self-Attention 结构，其中用到的 Q, K, V矩阵通过输出进行线性变换得到。</li>
<li>Transformer 中 Multi-Head Attention 中有多个 Self-Attention，可以捕获单词之间多种维度上的相关系数 attention score。</li>
</ul>
<hr>
<p>来源：<a href="https://www.cnblogs.com/kongen/p/18088002">Transformer模型详解（图解最完整版）</a></p>
]]></content>
  </entry>
</search>
